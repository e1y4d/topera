{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/e1y4d/topera/blob/main/topera_last_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers\n",
        "!pip install annoy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnJNRA5lqfCD",
        "outputId": "b72eb33a-ad1c-46cb-ca88-44ad526274f6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence_transformers\n",
            "  Downloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.41.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.23.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.15.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.4.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence_transformers\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 sentence_transformers-3.0.1\n",
            "Collecting annoy\n",
            "  Downloading annoy-1.17.3.tar.gz (647 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m647.5/647.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: annoy\n",
            "  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for annoy: filename=annoy-1.17.3-cp310-cp310-linux_x86_64.whl size=552448 sha256=7b59ef9316b59f5bd7c367344e0f6b39bb7bbee82f802243ae5743ac5adf9799\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/8a/da/f714bcf46c5efdcfcac0559e63370c21abe961c48e3992465a\n",
            "Successfully built annoy\n",
            "Installing collected packages: annoy\n",
            "Successfully installed annoy-1.17.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9G_x2YTjBoBl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8de8d985-dad9-43b4-80be-83fb5abac22b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from annoy import AnnoyIndex\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "S-U_X1n5o6ts",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a0eba9c-300d-4728-97ef-15019ba1af59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "##--uncomment if not using colab\n",
        "''' with open('user_response.json', 'r') as json_file:\n",
        "    jsondata = json.load(json_file)\n",
        "    print(\"Original data:\", jsondata) '''\n",
        "\n",
        "##--comment if not using colab\n",
        "drive.mount('/content/drive')\n",
        "with open('/content/drive/MyDrive/user_response.json', 'r') as json_file:\n",
        "    jsondata = json.load(json_file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "le = LabelEncoder()\n",
        "data = pd.DataFrame(jsondata).dropna().copy()\n",
        "global recommendations, k"
      ],
      "metadata": {
        "id": "h5jkVpTssa4Q"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "JIW62W16SWa5",
        "outputId": "f8124cb3-db62-4fff-9da3-8aa532b07cff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'React': ['6a42e8d3-3e26-41b7-9a5c-0a1f1f2a5ec0',\n",
              "  'f63b22fc-2de8-41a1-a45a-c34da9e547a5']}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "def recommend_collaborators(user_id, k=5):\n",
        "\n",
        "    recommendations = {}\n",
        "    if user_id not in data['UserId'].tolist():\n",
        "        print(f\"User with ID {user_id} not found in data.\")\n",
        "        return recommendations  # Empty dictionary\n",
        "\n",
        "    user_technology = data[data[\"UserId\"] == user_id][[\"TechnologyOfInterest\"]]\n",
        "    for technology in user_technology[\"TechnologyOfInterest\"].explode().unique():\n",
        "        filtered_users = data[data['TechnologyLearned'] == technology]\n",
        "        filtered_users = filtered_users[filtered_users['UserId'] != user_id]\n",
        "\n",
        "        if len(filtered_users) == 0:\n",
        "            recommendations[technology] = \"No users found for this technology\"\n",
        "            continue\n",
        "\n",
        "        filtered = filtered_users.drop(columns=['TechnologyLearned', 'TechnologyOfInterest']).copy()\n",
        "        filtered['UserId'] = filtered['UserId'].astype(str)  # Ensure string user IDs\n",
        "        encoded_data = filtered.drop(columns=['UserId'])\n",
        "        encoded_data = encoded_data.apply(le.fit_transform, axis=0)\n",
        "        similarity_matrix = euclidean_distances(encoded_data)\n",
        "        similar_df = pd.DataFrame({'UserId': filtered['UserId'], 'Score': similarity_matrix[0]})\n",
        "        similar_df = similar_df.sort_values(by='Score', ascending=True)\n",
        "        similar_df = similar_df['UserId'].tolist()  # Select only User IDs\n",
        "        recommendations.setdefault(technology, similar_df)[:k] = similar_df[:k]\n",
        "\n",
        "    return recommendations\n",
        "\n",
        "# Example usage\n",
        "recommend_collaborators('e17bda85-98e1-4dc5-a7e5-3fd790116fcf')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.head())"
      ],
      "metadata": {
        "id": "1UBg7IY2xgGT",
        "outputId": "42b45b7d-5c13-4b72-95aa-3dd6136bb5b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  PreferredLanguage  TrackLearned TechnologyLearned    TrackLevel  \\\n",
            "0           English    Full-stack         .NET Core  Intermediate   \n",
            "1            Arabic      Frontend             React      Advanced   \n",
            "2           English  Data Science            Python      Beginner   \n",
            "3           English       Backend         .NET Core  Intermediate   \n",
            "4           English    Full-stack           Angular      Advanced   \n",
            "\n",
            "  ReferralSource EmploymentStatus BasicProgrammingLanguagesKnowledge  \\\n",
            "0   Social media         Employed                     C#, JavaScript   \n",
            "1         Friend       Unemployed              HTML, CSS, JavaScript   \n",
            "2  Search engine          Student                             Python   \n",
            "3   Social media         Employed                            C#, SQL   \n",
            "4         Friend         Employed  JavaScript, TypeScript, HTML, CSS   \n",
            "\n",
            "  ProficientProgrammingLanguages PreferredLearningStyle LearningFrequency  \\\n",
            "0                             C#                 Videos             Daily   \n",
            "1                     JavaScript                  Books            Weekly   \n",
            "2                                             Tutorials             Daily   \n",
            "3                             C#                 Videos            Weekly   \n",
            "4         JavaScript, TypeScript                 Videos             Daily   \n",
            "\n",
            "  PreferredCommunicationMethod TechnologyOfInterest  \\\n",
            "0                         Chat              [React]   \n",
            "1                        Email             [Vue.js]   \n",
            "2                   Video call  [Machine, Learning]   \n",
            "3                         Chat      [ASP.NET, Core]   \n",
            "4                         Chat            [Node.js]   \n",
            "\n",
            "   ShiftingFromAnotherCareer  WeeklyHoursDedicatedToLearningAndCollaboration  \\\n",
            "0                      False                                              15   \n",
            "1                       True                                              10   \n",
            "2                      False                                              20   \n",
            "3                      False                                              15   \n",
            "4                      False                                              10   \n",
            "\n",
            "  MotivationForLearningAndCollaboration         GoalsOnThePlatform  \\\n",
            "0                    Career advancement        Learning new skills   \n",
            "1                     Skill development  Finding job opportunities   \n",
            "2                     Personal interest                 Networking   \n",
            "3                     Skill development        Learning new skills   \n",
            "4                     Skill development                 Networking   \n",
            "\n",
            "   ComfortLevelWithRemoteWorkOrCollaboration ProjectTypeInterest  \\\n",
            "0                                       True           Freelance   \n",
            "1                                       True         Open-source   \n",
            "2                                       True            Personal   \n",
            "3                                       True           Freelance   \n",
            "4                                       True           Freelance   \n",
            "\n",
            "                                 UserId  \n",
            "0  e46d51bf-3680-4e38-9ee9-9721787317b4  \n",
            "1  6a42e8d3-3e26-41b7-9a5c-0a1f1f2a5ec0  \n",
            "2  7e95cf13-2f18-45af-90d1-02e37cb0e83c  \n",
            "3  c7f17e01-7a4b-4722-9857-8ef972e6c971  \n",
            "4  8e7cb4ae-daae-4969-8f05-35af9ff47d0d  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#method 2.\n",
        "#data['TechnologyOfInterest'] = data['TechnologyOfInterest'].apply(lambda x: x.split())\n",
        "\n",
        "# --- Sentence Embedding ---\n",
        "model = SentenceTransformer('all-mpnet-base-v2')\n",
        "all_technologies = set(t for techs in data['TechnologyOfInterest'] for t in techs)\n",
        "technology_embeddings = model.encode(list(all_technologies))\n",
        "\n",
        "dim = technology_embeddings.shape[1]\n",
        "ann_index = AnnoyIndex(dim, 'angular')\n",
        "for i, embedding in enumerate(technology_embeddings):\n",
        "    ann_index.add_item(i, embedding)\n",
        "ann_index.build(10)\n",
        "\n",
        "# --- Recommendation Function ---\n",
        "def recommend_collaborators(user_id, data, k=5):\n",
        "    if user_id not in data['UserId'].tolist():\n",
        "        print(f\"User with ID {user_id} not found in data.\")\n",
        "        return {}\n",
        "\n",
        "    user_technologies = data[data[\"UserId\"] == user_id][\"TechnologyOfInterest\"].tolist()[0]\n",
        "    other_users = data[data['UserId'] != user_id]\n",
        "\n",
        "    recommendations = {}\n",
        "    for technology in user_technologies:\n",
        "        user_embedding = model.encode([technology])\n",
        "        potential_match_indices, distances = ann_index.get_nns_by_vector(user_embedding[0], 10, include_distances=True)\n",
        "        potential_matches = other_users.iloc[potential_match_indices]\n",
        "        other_user_embeddings = model.encode(potential_matches['TechnologyOfInterest'].apply(lambda x: ' '.join(x)).tolist())\n",
        "        similarities = util.cos_sim(user_embedding, other_user_embeddings)[0].tolist()\n",
        "        similar_df = pd.DataFrame({'UserId': potential_matches['UserId'], 'Score': similarities}).sort_values(by='Score', ascending=False)\n",
        "        top_users = similar_df['UserId'].tolist()[:k]\n",
        "        recommendations[technology] = top_users if top_users else [\"No users found for this technology\"]\n",
        "\n",
        "    return recommendations\n",
        "\n",
        "# --- Example Usage ---\n",
        "recommend_collaborators('e17bda85-98e1-4dc5-a7e5-3fd790116fcf', data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dypAZMh5ewRB",
        "outputId": "c1c5768d-8406-4e53-d593-ea628784421f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'React': ['ad70ff26-d4d2-4e46-a789-305fc4f6b647',\n",
              "  'e46d51bf-3680-4e38-9ee9-9721787317b4',\n",
              "  'd72c4a24-5f9c-49d0-b8cb-d993cb47ef8b',\n",
              "  '2e84d2b1-2317-4b9b-a4d1-1ef47770f4dc',\n",
              "  '6a42e8d3-3e26-41b7-9a5c-0a1f1f2a5ec0']}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IHPdsixMfTmH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}